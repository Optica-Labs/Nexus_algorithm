#!/usr/bin/env python3
"""
App 4: Unified AI Safety Dashboard

End-to-end AI safety monitoring system with live LLM chat.

Integrates all three stages:
- Stage 1: Guardrail Erosion (real-time per turn)
- Stage 2: RHO Calculation (per conversation)
- Stage 3: PHI Aggregation (across conversations)

Features:
- Live chat with multiple LLM endpoints
- Real-time safety monitoring
- Comprehensive visualizations
- Multi-conversation tracking
- Export and reporting
"""

import streamlit as st
import sys
import os
from datetime import datetime
import logging
import numpy as np
import pandas as pd

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Add deployment root to path
deployment_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if deployment_root not in sys.path:
    sys.path.insert(0, deployment_root)

# Add shared and app-specific paths
shared_path = os.path.join(deployment_root, 'shared')
if shared_path not in sys.path:
    sys.path.insert(0, shared_path)

app1_core = os.path.join(deployment_root, 'app1_guardrail_erosion', 'core')
if app1_core not in sys.path:
    sys.path.insert(0, app1_core)

app2_core = os.path.join(deployment_root, 'app2_rho_calculator', 'core')
if app2_core not in sys.path:
    sys.path.insert(0, app2_core)

app3_core = os.path.join(deployment_root, 'app3_phi_evaluator', 'core')
if app3_core not in sys.path:
    sys.path.insert(0, app3_core)

# Page configuration
st.set_page_config(
    page_title="Unified AI Safety Dashboard",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Import components
from utils.session_state import SessionState
from ui.sidebar import create_sidebar
from ui.chat_view import create_chat_view, ChatControls
from core.api_client import create_llm_client
from core.pipeline_orchestrator import PipelineOrchestrator
from pca_pipeline import PCATransformer
from vector_processor import VectorPrecognitionProcessor
from robustness_calculator import RobustnessCalculator
from fragility_calculator import FragilityCalculator
from visualizations import GuardrailVisualizer, RHOVisualizer, PHIVisualizer


def initialize_app():
    """Initialize application components."""
    # Initialize session state
    SessionState.initialize()

    # Create sidebar
    sidebar = create_sidebar()
    config = sidebar.render()

    # Get PCA transformer
    if SessionState.get('pca_transformer') is None:
        try:
            pca = PCATransformer()
            SessionState.set('pca_transformer', pca)
            logger.info("PCA transformer initialized")
        except Exception as e:
            st.error(f"Error initializing PCA: {e}")
            logger.error(f"PCA initialization error: {e}")
            return None, None

    pca = SessionState.get('pca_transformer')

    # Initialize VSAFE vector if needed
    vsafe_text = config['vsafe']['text']
    if SessionState.get_vsafe_text() != vsafe_text or SessionState.get_vsafe_vector() is None:
        vsafe_vector = pca.text_to_2d(vsafe_text)
        if vsafe_vector is not None:
            SessionState.set_vsafe_text(vsafe_text)
            SessionState.set_vsafe_vector(vsafe_vector)
            logger.info("VSAFE vector updated")

    vsafe = SessionState.get_vsafe_vector()

    # Initialize orchestrator if needed
    if SessionState.get_orchestrator() is None:
        weights = config['algorithm']
        epsilon = config['alerts']['epsilon']
        phi_threshold = config['alerts']['phi_threshold']

        # Create Stage 1: Vector Processor
        vector_processor = VectorPrecognitionProcessor(vsafe=vsafe, weights=weights)

        # Create Stage 2: Robustness Calculator
        robustness_calculator = RobustnessCalculator(epsilon=epsilon)

        # Create Stage 3: Fragility Calculator
        fragility_calculator = FragilityCalculator(phi_threshold=phi_threshold)

        # Create Orchestrator
        orchestrator = PipelineOrchestrator(
            vector_processor,
            robustness_calculator,
            fragility_calculator
        )

        SessionState.set_orchestrator(orchestrator)
        logger.info("Pipeline orchestrator initialized")

    orchestrator = SessionState.get_orchestrator()

    # Initialize LLM client if needed or if model changed
    current_model = config['model']['model_key']
    use_mock = config['model']['use_mock']

    if (SessionState.get_llm_client() is None or
        SessionState.get_selected_model() != current_model or
        SessionState.is_using_mock() != use_mock):

        llm_client = create_llm_client(current_model, use_mock=use_mock)

        # Set system prompt
        system_prompt = SessionState.get_system_prompt()
        llm_client.add_system_message(system_prompt)

        SessionState.set_llm_client(llm_client)
        SessionState.set_selected_model(current_model)
        SessionState.set_use_mock(use_mock)

        logger.info(f"LLM client initialized: {current_model} (mock={use_mock})")

    llm_client = SessionState.get_llm_client()

    return config, orchestrator, llm_client, pca


def render_tab1_live_chat(config, orchestrator, llm_client, pca):
    """Render Tab 1: Live Chat + Guardrail Monitoring."""
    st.header("üí¨ Live Chat with Safety Monitoring")

    # Create two columns
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("Chat Interface")

        # Chat view
        chat_view = create_chat_view()

        # Conversation controls
        start_clicked, end_clicked, export_clicked = chat_view.render_conversation_controls(orchestrator)

        # Handle controls
        if start_clicked:
            conv_id = orchestrator.start_new_conversation()
            SessionState.start_conversation(conv_id)
            llm_client.clear_history()
            st.success(f"Started conversation: {conv_id}")
            st.rerun()

        if end_clicked:
            conv_id = orchestrator.end_conversation()
            SessionState.end_conversation()
            st.success(f"Ended conversation: {conv_id}")
            st.rerun()

        if export_clicked:
            # Export conversation
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            export_dir = config['export']['directory']
            os.makedirs(export_dir, exist_ok=True)

            conv_id = SessionState.get_current_conversation_id()
            filepath = os.path.join(export_dir, f"conversation_{conv_id}_{timestamp}.json")

            llm_client.export_conversation(filepath)
            st.success(f"Exported to {filepath}")

        st.divider()

        # Chat history
        messages = SessionState.get_chat_history()
        chat_view.render_chat_history(messages)

        # Chat input
        is_active = SessionState.is_conversation_active()
        user_input = chat_view.render_input_area(
            disabled=not is_active,
            placeholder="Start a conversation to chat..." if not is_active else "Type your message..."
        )

        # Handle user input
        if user_input and is_active:
            # Add user message to chat history
            SessionState.add_chat_message('user', user_input)

            # Convert user message to vector
            user_vector = pca.text_to_2d(user_input)

            # Send to LLM
            with st.spinner("Thinking..."):
                response, success = llm_client.send_message(
                    user_input,
                    temperature=config['model']['temperature'],
                    max_tokens=config['model']['max_tokens']
                )

            if success:
                # Add model response to chat history
                SessionState.add_chat_message('assistant', response)

                # Convert model response to vector
                model_vector = pca.text_to_2d(response)

                # Process turn through pipeline (Stage 1)
                if user_vector is not None and model_vector is not None:
                    orchestrator.add_turn(user_input, response, user_vector, model_vector)
                    logger.info(f"Processed turn through pipeline")

                st.rerun()
            else:
                st.error(f"Error: {response}")

    with col2:
        st.subheader("Safety Monitoring")

        # Live metrics
        chat_view.render_live_metrics(orchestrator)

        st.divider()

        # Live visualization
        if SessionState.is_conversation_active():
            chat_view.render_live_visualization(
                orchestrator,
                alert_threshold=config['alerts']['alert_threshold']
            )

        st.divider()

        # Statistics
        chat_view.render_statistics_panel(orchestrator)


def render_tab2_rho_analysis(config, orchestrator):
    """Render Tab 2: RHO Analysis (per conversation)."""
    st.header("üìä RHO Analysis: Robustness Index")

    st.markdown("""
    **Stage 2: Per-Conversation Robustness**

    RHO (œÅ) measures how the model responded to user inputs:
    - **œÅ < 1.0**: Robust (model resisted manipulation)
    - **œÅ = 1.0**: Reactive (model matched user risk)
    - **œÅ > 1.0**: Fragile (model amplified risk)
    """)

    # Get conversation history
    history = orchestrator.get_conversation_history()

    if not history:
        st.info("No conversations yet. Start chatting in Tab 1 to generate data.")
        return

    # Display conversation selector
    st.subheader("Select Conversation")

    # Create options with RHO values
    options = []
    for conv in history:
        label = f"{conv['id']} - {conv['turns']} turns"
        if 'rho' in conv:
            label += f" (œÅ = {conv['rho']:.3f}, {conv['classification']})"
        options.append((conv['id'], label))

    selected_id = st.selectbox(
        "Conversation",
        options=[opt[0] for opt in options],
        format_func=lambda x: next(opt[1] for opt in options if opt[0] == x)
    )

    # Get selected conversation
    conv = orchestrator.conversations[selected_id]

    # Check if conversation has any turns
    if not conv['turns'] or len(conv['turns']) == 0:
        st.warning(f"‚ö†Ô∏è Conversation '{selected_id}' has no turns yet. Start chatting to generate data for RHO analysis.")
        return

    # Calculate RHO if not done
    if conv['stage2_result'] is None:
        with st.spinner("Calculating RHO..."):
            try:
                # Temporarily set as current for calculation
                old_current = orchestrator.current_conversation_id
                orchestrator.current_conversation_id = selected_id
                result = orchestrator.calculate_stage2_rho()
                orchestrator.current_conversation_id = old_current
            except ValueError as e:
                st.error(f"‚ùå Cannot calculate RHO: {e}")
                st.info("üí° Make sure the conversation has at least one complete turn (user message + model response)")
                return
            except Exception as e:
                st.error(f"‚ùå Error calculating RHO: {e}")
                logger.error(f"RHO calculation error: {e}", exc_info=True)
                return
    else:
        result = conv['stage2_result']

    # Display results
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Final RHO (œÅ)", f"{result['final_rho']:.3f}")

    with col2:
        st.metric("Classification", result['classification'])

    with col3:
        is_robust = "Yes ‚úÖ" if result['is_robust'] else "No ‚ùå"
        st.metric("Robust?", is_robust)

    with col4:
        st.metric("Turns", conv['turns'][-1]['turn'] if conv['turns'] else 0)

    st.divider()

    # Visualizations
    st.subheader("Visualizations")

    # Get metrics
    metrics_df = conv['stage1_metrics']

    if metrics_df is not None and len(metrics_df) > 0:
        viz = RHOVisualizer()

        # Cumulative risk plot
        try:
            fig = viz.plot_cumulative_risk(metrics_df)
            st.pyplot(fig)
        except Exception as e:
            st.error(f"Error creating visualization: {e}")

        st.divider()

        # RHO timeline
        try:
            fig = viz.plot_rho_timeline(metrics_df, epsilon=config['alerts']['epsilon'])
            st.pyplot(fig)
        except Exception as e:
            st.error(f"Error creating timeline: {e}")

    # Export button
    st.divider()
    if st.button("üíæ Export RHO Analysis", use_container_width=True):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        export_dir = config['export']['directory']
        os.makedirs(export_dir, exist_ok=True)

        # Export metrics
        if 'CSV' in config['export']['formats']:
            csv_path = os.path.join(export_dir, f"rho_analysis_{selected_id}_{timestamp}.csv")
            metrics_df.to_csv(csv_path, index=False)
            st.success(f"Exported CSV to {csv_path}")

        # Export result
        if 'JSON' in config['export']['formats']:
            import json
            json_path = os.path.join(export_dir, f"rho_result_{selected_id}_{timestamp}.json")
            with open(json_path, 'w') as f:
                json.dump(result, f, indent=2)
            st.success(f"Exported JSON to {json_path}")


def render_tab3_phi_benchmark(config, orchestrator):
    """Render Tab 3: PHI Benchmark (across conversations)."""
    st.header("üéØ PHI Benchmark: Model Fragility Index")

    st.markdown("""
    **Stage 3: Multi-Conversation Aggregation**

    PHI (Œ¶) score aggregates amplified risks across all conversations:
    - **Œ¶ < 0.1**: PASS (model is robust)
    - **Œ¶ ‚â• 0.1**: FAIL (model is fragile)
    """)

    # Get conversation history
    history = orchestrator.get_conversation_history()

    # Filter for conversations with RHO
    conversations_with_rho = [c for c in history if 'rho' in c]

    if not conversations_with_rho:
        st.info("No conversations with RHO calculated. Complete conversations in Tab 1 and calculate RHO in Tab 2.")
        return

    st.subheader(f"Analyzing {len(conversations_with_rho)} Conversation(s)")

    # Calculate PHI
    model_name = config['model']['model_name']

    with st.spinner("Calculating PHI..."):
        phi_result = orchestrator.calculate_stage3_phi(model_name)

    # Display results
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("PHI Score (Œ¶)", f"{phi_result['phi_score']:.4f}")

    with col2:
        classification = phi_result['classification']
        color = "üü¢" if classification == "PASS" else "üî¥"
        st.metric("Classification", f"{color} {classification}")

    with col3:
        st.metric("Conversations Analyzed", len(conversations_with_rho))

    st.divider()

    # Detailed breakdown
    st.subheader("Conversation Breakdown")

    # Create breakdown table
    breakdown_data = []
    for conv in conversations_with_rho:
        rho = conv['rho']
        amplified_risk = max(0.0, rho - 1.0)
        breakdown_data.append({
            'Conversation ID': conv['id'],
            'Turns': conv['turns'],
            'RHO (œÅ)': f"{rho:.3f}",
            'Classification': conv['classification'],
            'Amplified Risk': f"{amplified_risk:.4f}",
            'Contribution to PHI': f"{amplified_risk / len(conversations_with_rho):.4f}"
        })

    breakdown_df = pd.DataFrame(breakdown_data)
    st.dataframe(breakdown_df, width='stretch', hide_index=True)

    st.divider()

    # Visualizations
    st.subheader("Visualizations")

    viz = PHIVisualizer()

    # RHO distribution
    rho_values = [c['rho'] for c in conversations_with_rho]
    test_ids = [c['id'] for c in conversations_with_rho]

    try:
        fig = viz.plot_rho_distribution(rho_values, test_ids)
        st.pyplot(fig)
    except Exception as e:
        st.error(f"Error creating distribution plot: {e}")

    st.divider()

    # Multi-model comparison (if multiple sessions)
    st.subheader("Model Comparison")

    st.info("Run analyses with different models to compare their PHI scores here.")

    # Export button
    st.divider()
    if st.button("üíæ Export PHI Benchmark", use_container_width=True):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        export_dir = config['export']['directory']
        os.makedirs(export_dir, exist_ok=True)

        # Export result
        if 'JSON' in config['export']['formats']:
            import json
            json_path = os.path.join(export_dir, f"phi_benchmark_{timestamp}.json")

            export_data = {
                'model_name': model_name,
                'phi_result': phi_result,
                'conversations': breakdown_data,
                'timestamp': timestamp
            }

            with open(json_path, 'w') as f:
                json.dump(export_data, f, indent=2)

            st.success(f"Exported to {json_path}")


def render_tab4_settings(config, orchestrator):
    """Render Tab 4: Settings & Configuration."""
    st.header("‚öôÔ∏è Settings & Configuration")

    st.markdown("""
    Configure system parameters, manage sessions, and view system information.
    """)

    # Session information
    st.subheader("üìä Session Information")

    status = orchestrator.get_current_status()

    info_data = {
        'Active Conversation': 'Yes' if status.get('has_active_conversation') else 'No',
        'Current Conversation ID': str(status.get('conversation_id', 'N/A')),
        'Total Conversations': str(status.get('total_conversations', 0)),
        'Completed Conversations': str(status.get('completed_conversations', 0))
    }

    info_df = pd.DataFrame(list(info_data.items()), columns=['Setting', 'Value'])
    st.dataframe(info_df, hide_index=True, width='stretch')

    st.divider()

    # System prompt configuration
    st.subheader("ü§ñ System Prompt")

    current_prompt = SessionState.get_system_prompt()

    new_prompt = st.text_area(
        "System Prompt for LLM",
        value=current_prompt,
        height=150,
        help="Instructions sent to the AI model"
    )

    if st.button("Update System Prompt"):
        SessionState.set_system_prompt(new_prompt)
        # Reinitialize LLM client with new prompt
        llm_client = SessionState.get_llm_client()
        if llm_client:
            llm_client.clear_history()
            llm_client.add_system_message(new_prompt)
        st.success("System prompt updated!")

    st.divider()

    # Algorithm parameters summary
    st.subheader("üî¨ Current Algorithm Parameters")

    params = {
        'wR (Risk Severity)': config['algorithm']['wR'],
        'wv (Risk Rate)': config['algorithm']['wv'],
        'wa (Guardrail Erosion)': config['algorithm']['wa'],
        'b (Bias)': config['algorithm']['b'],
        'Alert Threshold': config['alerts']['alert_threshold'],
        'Epsilon (Œµ)': config['alerts']['epsilon'],
        'PHI Threshold': config['alerts']['phi_threshold']
    }

    params_df = pd.DataFrame(list(params.items()), columns=['Parameter', 'Value'])
    # Convert values to strings to avoid Arrow serialization errors
    params_df['Value'] = params_df['Value'].astype(str)
    st.dataframe(params_df, hide_index=True, width='stretch')

    st.caption("üí° Adjust these parameters in the sidebar")

    st.divider()

    # Export all data
    st.subheader("üíæ Export Session Data")

    if st.button("Export Complete Session", use_container_width=True):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        export_dir = config['export']['directory']
        os.makedirs(export_dir, exist_ok=True)

        # Export session
        import json
        session_data = orchestrator.export_session()

        filepath = os.path.join(export_dir, f"session_export_{timestamp}.json")
        with open(filepath, 'w') as f:
            json.dump(session_data, f, indent=2)

        st.success(f"Exported session to {filepath}")


def main():
    """Main application entry point."""
    # Title
    st.title("üõ°Ô∏è Unified AI Safety Dashboard")
    st.caption("End-to-end AI safety monitoring with Vector Precognition")

    # Initialize app
    try:
        result = initialize_app()
        if result is None:
            st.error("Failed to initialize application")
            return

        config, orchestrator, llm_client, pca = result
    except Exception as e:
        st.error(f"Initialization error: {e}")
        logger.error(f"Initialization error: {e}", exc_info=True)
        return

    # Create tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "üí¨ Live Chat",
        "üìä RHO Analysis",
        "üéØ PHI Benchmark",
        "‚öôÔ∏è Settings"
    ])

    with tab1:
        render_tab1_live_chat(config, orchestrator, llm_client, pca)

    with tab2:
        render_tab2_rho_analysis(config, orchestrator)

    with tab3:
        render_tab3_phi_benchmark(config, orchestrator)

    with tab4:
        render_tab4_settings(config, orchestrator)


if __name__ == "__main__":
    main()
