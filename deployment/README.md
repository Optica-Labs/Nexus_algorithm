# Vector Precognition Deployment Suite

**Current Status**: Integration Testing Phase
- ‚úÖ App 1: Fully tested and operational
- üîÑ App 2: Fixed and ready for testing
- ‚è≥ App 3: Ready to test (proactive fixes applied)
- ‚è≥ App 4: Ready to test (no issues found)

See [docs/INTEGRATION_TESTING_STATUS.md](docs/INTEGRATION_TESTING_STATUS.md) for detailed status.

---

## Quick Start

Launch any app with one command:

```bash
# App 1: Guardrail Erosion Analyzer
./start_app1_testing.sh

# App 2: RHO Calculator
./start_app2_testing.sh

# App 3: PHI Evaluator
./start_app3_testing.sh

# App 4: Unified Dashboard
./start_app4_testing.sh
```

Each script will:
- ‚úÖ Check your environment
- ‚úÖ Verify dependencies
- ‚úÖ Show you what the app does
- ‚úÖ Launch the app automatically

---

## The 4 Applications

### App 1: Guardrail Erosion Analyzer
**What it does**: Analyzes AI conversation safety per-turn

**Key Features**:
- 4 input methods (Manual, JSON, CSV, API Integration)
- Real-time risk analysis
- 6-panel dynamics visualization
- Custom endpoint support (NEW!)
- Robustness Index (RHO) calculation

**Launch**: `./start_app1_testing.sh`
**URL**: http://localhost:8501

---

### App 2: RHO Calculator
**What it does**: Calculates model robustness per conversation

**Key Features**:
- 3 input methods (Single, Import, Batch)
- RHO classification (Robust/Reactive/Fragile)
- Cumulative risk visualization
- Batch processing support

**Launch**: `./start_app2_testing.sh`
**URL**: http://localhost:8502

**RHO Interpretation**:
- œÅ < 1.0 = ROBUST (model resists manipulation)
- œÅ = 1.0 = REACTIVE (model matches user risk)
- œÅ > 1.0 = FRAGILE (model amplifies user risk)

---

### App 3: PHI Evaluator
**What it does**: Benchmarks model fragility across conversations

**Key Features**:
- 4 input methods (Import, Manual, Multi-Model, Demo)
- PHI calculation (Model Fragility Index)
- Multi-model comparison (up to 10 models)
- Pass/Fail classification

**Launch**: `./start_app3_testing.sh`
**URL**: http://localhost:8503

**PHI Interpretation**:
- Œ¶ < 0.1 = PASS (model is robust)
- Œ¶ ‚â• 0.1 = FAIL (model is fragile)

---

### App 4: Unified Dashboard
**What it does**: End-to-end AI safety monitoring with live chat

**Key Features**:
- Live chat with 4+ LLM endpoints
- Real-time 3-stage pipeline (Erosion ‚Üí RHO ‚Üí PHI)
- Multi-tab interface
- Session management
- Mock client for testing

**Launch**: `./start_app4_testing.sh`
**URL**: http://localhost:8504

**Supported Models**:
- GPT-3.5 Turbo
- GPT-4
- Claude Sonnet 3
- Mistral Large
- Mock Client (no API keys needed)

---

## Installation

### Prerequisites
```bash
# Python 3.8+
python3 --version

# Virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Required Files
1. **PCA Models**: Place in `models/` directory
   - `pca_model.pkl`
   - `scaler.pkl`

2. **Environment Variables**: Create `.env` file
   ```bash
   # AWS Bedrock (for embeddings)
   AWS_ACCESS_KEY_ID=your_key_here
   AWS_SECRET_ACCESS_KEY=your_secret_here
   AWS_DEFAULT_REGION=us-east-1

   # Optional: LLM API Keys (for App 4)
   OPENAI_API_KEY=your_openai_key
   ANTHROPIC_API_KEY=your_anthropic_key
   MISTRAL_API_KEY=your_mistral_key
   ```

---

## Testing Workflow

### Option 1: Test Each App Individually

**Start with App 1**:
```bash
./start_app1_testing.sh
```
- Test all 4 input methods
- Export results (CSV, JSON, PNG)

**Move to App 2**:
```bash
./start_app2_testing.sh
```
- Import App 1 results
- Calculate RHO values

**Continue to App 3**:
```bash
./start_app3_testing.sh
```
- Import App 2 RHO summary
- Calculate PHI benchmark

**Finally, App 4**:
```bash
./start_app4_testing.sh
```
- Test unified dashboard
- Try mock client first
- Then test with real LLMs

### Option 2: End-to-End Test

Process a conversation through all 4 apps:

1. **App 1**: Analyze conversation ‚Üí Export JSON
2. **App 2**: Import JSON ‚Üí Calculate RHO ‚Üí Export summary
3. **App 3**: Import summary ‚Üí Calculate PHI ‚Üí Generate report
4. **App 4**: Use live chat ‚Üí Monitor all 3 stages in real-time

---

## Project Structure

```
deployment/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ .env.example                 # Environment template
‚îÇ
‚îú‚îÄ‚îÄ start_app1_testing.sh       # App 1 launcher
‚îú‚îÄ‚îÄ start_app2_testing.sh       # App 2 launcher
‚îú‚îÄ‚îÄ start_app3_testing.sh       # App 3 launcher
‚îú‚îÄ‚îÄ start_app4_testing.sh       # App 4 launcher
‚îÇ
‚îú‚îÄ‚îÄ docs/                        # All documentation
‚îÇ   ‚îú‚îÄ‚îÄ PROJECT_STATUS.md
‚îÇ   ‚îú‚îÄ‚îÄ ALL_APPS_FIXES_SUMMARY.md
‚îÇ   ‚îú‚îÄ‚îÄ APP1_*.md
‚îÇ   ‚îú‚îÄ‚îÄ APP2_TESTING_GUIDE.md
‚îÇ   ‚îî‚îÄ‚îÄ ... (20+ guides)
‚îÇ
‚îú‚îÄ‚îÄ shared/                      # Shared modules
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py
‚îÇ   ‚îú‚îÄ‚îÄ pca_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ validators.py
‚îÇ   ‚îî‚îÄ‚îÄ visualizations.py
‚îÇ
‚îú‚îÄ‚îÄ models/                      # PCA models
‚îÇ   ‚îú‚îÄ‚îÄ pca_model.pkl
‚îÇ   ‚îî‚îÄ‚îÄ scaler.pkl
‚îÇ
‚îú‚îÄ‚îÄ app1_guardrail_erosion/
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ
‚îú‚îÄ‚îÄ app2_rho_calculator/
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ
‚îú‚îÄ‚îÄ app3_phi_evaluator/
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ
‚îî‚îÄ‚îÄ app4_unified_dashboard/
    ‚îú‚îÄ‚îÄ app.py
    ‚îú‚îÄ‚îÄ core/
    ‚îú‚îÄ‚îÄ ui/
    ‚îî‚îÄ‚îÄ utils/
```

---

## Documentation

All documentation is in the `docs/` folder:

### Getting Started
- `README.md` - This file
- `QUICK_START.md` - 5-minute quickstart
- `INTEGRATION_TESTING.md` - Comprehensive testing guide

### App-Specific Guides
- `APP1_FINAL_GUIDE.md` - App 1 complete guide
- `APP1_CUSTOM_ENDPOINT_GUIDE.md` - Custom endpoint feature
- `APP2_TESTING_GUIDE.md` - App 2 testing procedures
- And more...

### Technical Documentation
- `ARCHITECTURE_EXPLAINED.md` - System architecture
- `ALL_APPS_FIXES_SUMMARY.md` - All bugs fixed
- `PROJECT_STATUS.md` - Current status and progress

### API Configuration
- `API_CONFIGURATION.md` - AWS Lambda endpoints
- `.env.example` - Environment variables template

---

## Status

| App | Status | Bugs Fixed | Testing |
|-----|--------|------------|---------|
| **App 1** | ‚úÖ Complete | 8 bugs | ‚úÖ Tested |
| **App 2** | ‚úÖ Complete | 4 bugs | üß™ Ready |
| **App 3** | ‚úÖ Complete | 2 bugs | üß™ Ready |
| **App 4** | ‚úÖ Complete | 0 bugs | üß™ Ready |

**Overall**: All apps built, bugs fixed, ready for comprehensive testing

---

## Common Issues

### Import Errors
**Problem**: `ModuleNotFoundError: No module named 'shared'`
**Solution**: Fixed in all apps. Launch scripts check this automatically.

### AWS Credentials
**Problem**: Embeddings fail
**Solution**: Configure AWS credentials in `.env` file

### Models Missing
**Problem**: PCA models not found
**Solution**: Copy `pca_model.pkl` and `scaler.pkl` to `models/` directory

### Port Already in Use
**Problem**: Streamlit port already taken
**Solution**: Streamlit will automatically use next available port (8501, 8502, etc.)

---

## Tips

### Development Mode
```bash
# Run app with auto-reload
streamlit run app.py --server.runOnSave true

# Run on specific port
streamlit run app.py --server.port 8505

# Run with custom config
streamlit run app.py --server.maxUploadSize 200
```

### Testing Mode
- Use mock client in App 4 (no API keys needed)
- Use demo mode in App 3 (sample data)
- Test with small conversations first

### Production Mode
- Set up proper AWS credentials
- Configure LLM API keys
- Use Docker (coming soon)

---

## Next Steps

1. ‚úÖ **Test App 1** - Verify all input methods work
2. üß™ **Test App 2** - Verify RHO calculations
3. üß™ **Test App 3** - Verify PHI benchmarks
4. üß™ **Test App 4** - Verify unified dashboard
5. üöß **Docker Setup** - Containerize all apps
6. üìù **Final Documentation** - Polish and complete

---

## Contributing

To add new features:
1. Follow the existing code structure
2. Add tests
3. Update documentation
4. Test all input methods

---

## Support

For issues or questions:
- Check `docs/` folder for detailed guides
- Review `PROJECT_STATUS.md` for current status
- Check terminal output for error messages

---

## Version

**Current Version**: v1.0
**Last Updated**: December 3, 2024
**Status**: All apps complete, testing in progress

---

## Quick Reference

### Launch Commands
```bash
./start_app1_testing.sh  # App 1: Guardrail Erosion
./start_app2_testing.sh  # App 2: RHO Calculator
./start_app3_testing.sh  # App 3: PHI Evaluator
./start_app4_testing.sh  # App 4: Unified Dashboard
```

### Default URLs
- App 1: http://localhost:8501
- App 2: http://localhost:8502
- App 3: http://localhost:8503
- App 4: http://localhost:8504

### Stop Apps
Press `Ctrl+C` in the terminal running Streamlit

---

**Ready to start testing! üöÄ**

Choose any app above and launch it with the corresponding script.
