#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Implementation of the Vector Precognition algorithm based on the white paper:
"AI Safety and Accuracy Using Guardrail Erosion and Risk Velocity in Vector Space" 
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# Set up a clean plotting style
plt.style.use('seaborn-v0_8-whitegrid')


class VectorPrecogntion:
    """
    Implements the Vector Precognition algorithm to calculate AI conversational risk.

    This class processes a sequence of conversation vectors (v_n) against a 
    safe-harbor vector (vsafe) to calculate risk severity, rate, erosion, 
    and likelihood for each turn[cite: 9, 43].
    """
    
    def __init__(self, vsafe: np.ndarray, weights: Dict[str, float]):
        """
        Initializes the Vector Precognition processor.

        Args:
            vsafe (np.ndarray): The "Guardrail Centroid" VSAFE vector[cite: 59]. 
                                This implementation assumes it's a 2D vector
                                as per the prompt.
            weights (Dict[str, float]): A dictionary containing the weights 
                                       (wR, wv, wa) and bias (b) for the 
                                       Failure Potential Score z(N)[cite: 114, 121].
                                       Example: {'wR': 1.5, 'wv': 1.0, 'wa': 3.0, 'b': -2.0}
        """
        if vsafe.ndim != 1 or len(vsafe) != 2:
            raise ValueError("VSAFE must be a 2-dimensional numpy array (e.g., np.array([x, y])).")
        
        # Normalize VSAFE to a unit vector for consistent cosine calculations
        norm = np.linalg.norm(vsafe)
        if norm == 0:
            raise ValueError("VSAFE vector cannot be a zero vector.")
        self.vsafe: np.ndarray = vsafe / norm
        
        self.weights: Dict[str, float] = weights
        
        # Lists to store the history of metrics for the conversation
        self.v_n_vectors: List[np.ndarray] = []
        self.R_n_list: List[float] = []  # Risk Severity (Position) [cite: 61]
        self.v_n_list: List[float] = []  # Risk Rate (Velocity) [cite: 72]
        self.a_n_list: List[float] = []  # Guardrail Erosion (Acceleration) [cite: 79]
        self.z_n_list: List[float] = []  # Failure Potential Score [cite: 112]
        self.L_n_list: List[float] = []  # Likelihood of Risk [cite: 105]

    def _calculate_cosine_distance(self, v_n: np.ndarray) -> float:
        """
        Calculates the Cosine Distance between the turn vector V_N and VSAFE.
        This is the Risk Function R(N).
        
        Note: Cosine Distance = 1 - Cosine Similarity.
        - Returns 0 if vectors are identical.
        - Returns 1 if vectors are orthogonal.
        - Returns 2 if vectors are perfectly opposite.
        
        This aligns with the user's "dot product" description where 0 is 
        aligned and 2 is not aligned[cite: 66, 67].
        
        Args:
            v_n (np.ndarray): The 2D vector for the current conversational turn.

        Returns:
            float: The Cosine Distance, R(N).
        """
        v_n_norm = np.linalg.norm(v_n)
        
        # Handle zero vector case (semantically meaningless, max risk)
        if v_n_norm == 0:
            return 1.0  # Orthogonal (neutral risk)
            
        v_n_unit = v_n / v_n_norm
        
        # Since self.vsafe is already a unit vector, dot product is Cosine Similarity
        cosine_similarity = np.dot(v_n_unit, self.vsafe)
        
        # R(N) = 1 - Cosine Similarity 
        cosine_distance = 1.0 - cosine_similarity
        
        # Clip to ensure the value is within the theoretical bounds [0, 2]
        return np.clip(cosine_distance, 0.0, 2.0)

    def process_turn(self, v_n: np.ndarray):
        """
        Processes a single conversational turn vector.
        
        It calculates R(N), v(N), a(N), z(N), and L(N) for this turn
        and appends them to the conversation's history.

        Args:
            v_n (np.ndarray): The 2D vector coordinates for the current turn.
        """
        if v_n.ndim != 1 or len(v_n) != 2:
            raise ValueError("Turn vector v_n must be a 2-dimensional numpy array (e.g., np.array([x, y])).")
        
        self.v_n_vectors.append(v_n)
        
        # --- Step 1 & 2: Calculate R(N) (Position) ---
        R_n = self._calculate_cosine_distance(v_n)
        self.R_n_list.append(R_n)
        
        # --- Step 3 (First Derivative): Calculate v(N) (Velocity) ---
        # v(N) â‰ˆ R(N) - R(N-1) 
        if len(self.R_n_list) == 1:
            v_n_val = 0.0  # No velocity at the first turn
        else:
            v_n_val = self.R_n_list[-1] - self.R_n_list[-2]
        self.v_n_list.append(v_n_val)

        # --- Step 3 (Second Derivative): Calculate a(N) (Acceleration) ---
        # a(N) â‰ˆ v(N) - v(N-1) 
        if len(self.v_n_list) <= 1:
            a_n_val = 0.0 # No acceleration at the first turn
        else:
            # Handle turn 2, where v(N-1) is v(1) = 0.0
            if len(self.v_n_list) == 2:
                 a_n_val = self.v_n_list[-1] - 0.0
            else:
                 a_n_val = self.v_n_list[-1] - self.v_n_list[-2]
        self.a_n_list.append(a_n_val)
        
        # --- Likelihood Step 1: Calculate z(N) (Failure Potential) ---
        # z(N) = (wRâ‹…R(N)) + (wvâ‹…v(N)) + (waâ‹…a(N)) + b 
        z_n = (
            (self.weights['wR'] * R_n) +
            (self.weights['wv'] * v_n_val) +
            (self.weights['wa'] * a_n_val) +
            self.weights['b']
        )
        self.z_n_list.append(z_n)
        
        # --- Likelihood Step 2: Calculate L(N) (Logistic Function) ---
        # L(N) = 1 / (1 + e^(-z(N))) 
        # Use np.exp for numerical stability with large negative z_n
        L_n = 1.0 / (1.0 + np.exp(-z_n))
        self.L_n_list.append(L_n)

    def get_metrics(self) -> pd.DataFrame:
        """
        Returns all calculated metrics as a pandas DataFrame.

        Returns:
            pd.DataFrame: A DataFrame with columns for each metric per turn.
        """
        if not self.R_n_list:
            return pd.DataFrame()
            
        turns = list(range(1, len(self.R_n_list) + 1))
        
        data = {
            "Turn": turns,
            "RiskSeverity_R(N)": self.R_n_list,
            "RiskRate_v(N)": self.v_n_list,
            "ErosionVelocity_a(N)": self.a_n_list,
            "FailurePotential_z(N)": self.z_n_list,
            "Likelihood_L(N)": self.L_n_list,
            "Vector": self.v_n_vectors
        }
        return pd.DataFrame(data).set_index("Turn")

    def plot_conversation_dynamics(self, alert_threshold: float = 0.8, output_subdir: str = 'visuals'):
        """
        Generates a 4-panel plot visualizing the conversation dynamics,
        similar to Figure 5 in the white paper.
        
        Args:
            alert_threshold (float): The likelihood value to draw an alert line.
            output_subdir (str): Subdirectory within output/ to save plots (e.g., 'text', 'manual', 'visuals')
        """
        metrics = self.get_metrics()
        if metrics.empty:
            print("No data to plot. Process some turns first.")
            return

        turns = metrics.index.values
        
        fig, axes = plt.subplots(4, 1, figsize=(12, 18), sharex=True)
        fig.suptitle("Calculus of Drift: Risk Trajectory and Predictive Likelihood", fontsize=16, y=1.02)

        # 1. Risk Severity (Position)
        axes[0].plot(turns, metrics['RiskSeverity_R(N)'], 'o-', label="Risk Score (R_N) - Severity")
        peak_severity_turn = metrics['RiskSeverity_R(N)'].idxmax()
        peak_severity_val = metrics['RiskSeverity_R(N)'].max()
        axes[0].axhline(peak_severity_val, color='red', linestyle='--', alpha=0.5)
        axes[0].text(turns[0], peak_severity_val * 1.05, 
                     f"Peak Severity: {peak_severity_val:.2f} @ Turn {peak_severity_turn}",
                     color='red')
        axes[0].set_ylabel("Risk Severity (0-2)")
        axes[0].set_title("1. Risk Severity (Position): How bad is it right now?")
        axes[0].legend(loc="upper left")

        # 2. Risk Rate (Velocity)
        colors_v = ['orange' if v >= 0 else 'brown' for v in metrics['RiskRate_v(N)']]
        axes[1].bar(turns, metrics['RiskRate_v(N)'], 
                    color=colors_v, width=0.8, alpha=0.7, label="Drift Rate (dR/dN) - Velocity")
        axes[1].axhline(0, color='grey', linestyle='--', linewidth=1)
        axes[1].set_ylabel("Drift Rate")
        axes[1].set_title("2. Drift Rate (Velocity): How fast is risk accumulating?")
        axes[1].legend(loc="upper left")
        
        # 3. Guardrail Erosion (Acceleration)
        colors_a = ['red' if a >= 0 else 'darkred' for a in metrics['ErosionVelocity_a(N)']]
        axes[2].bar(turns, metrics['ErosionVelocity_a(N)'], 
                    color=colors_a, width=0.8, alpha=0.7, label="Erosion Velocity (dÂ²R/dNÂ²) - Acceleration")
        axes[2].axhline(0, color='grey', linestyle='--', linewidth=1)
        axes[2].set_ylabel("Erosion Velocity")
        axes[2].set_title("3. Erosion Velocity (Acceleration): Is risk increasing faster?")
        axes[2].legend(loc="upper left")

        # 4. Likelihood of Risk (Probability)
        axes[3].plot(turns, metrics['Likelihood_L(N)'], 'g*--', label="Likelihood of Risk (%)")
        axes[3].axhline(alert_threshold, color='red', linestyle=':', 
                        label=f"Alert Threshold (e.g., {alert_threshold*100:.0f}%)")
        axes[3].set_ylabel("Likelihood (%)")
        axes[3].set_title("4. Likelihood of Risk: What is the probability of a breach?")
        axes[3].set_xlabel("Conversation Turn (N)")
        axes[3].set_ylim(0, 1.05)
        axes[3].legend(loc="upper left")
        
        plt.tight_layout()

        # Ensure output directory exists and save the figure
        project_root = os.path.dirname(os.path.dirname(__file__))
        out_dir = os.path.join(project_root, 'output', output_subdir)
        os.makedirs(out_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.join(out_dir, f"conversation_dynamics_{timestamp}.png")
        try:
            fig.savefig(filename, bbox_inches='tight', dpi=150)
            print(f"âœ“ Saved conversation dynamics plot to: {filename}")
        except Exception as e:
            print(f"âœ— Failed to save conversation dynamics plot: {e}")

        plt.show()

def plot_conversation_summary(
    processed_conversations: List[VectorPrecogntion],
    id_list: Optional[List[str]] = None,
    output_subdir: str = 'visuals'
):
    """
    Generates the final X/Y grid plotting Risk Likelihood vs. Risk Severity.
    
    Each point on the plot represents the *peak* risk and *peak* likelihood
    from one entire conversation.

    Args:
        processed_conversations (List[VectorPrecogntion]): A list of
            VectorPrecogntion objects that have already processed a conversation.
        id_list (Optional[List[str]]): A list of string identifiers for each
            conversation, for labeling.
        output_subdir (str): Subdirectory within output/ to save plots (e.g., 'text', 'manual', 'visuals')
    """
    
    if id_list and len(id_list) != len(processed_conversations):
        raise ValueError("id_list must have the same length as processed_conversations.")

    severities = []
    likelihoods = []
    
    for convo in processed_conversations:
        metrics = convo.get_metrics()
        if not metrics.empty:
            # Get the peak severity and likelihood from the conversation
            peak_severity = metrics['RiskSeverity_R(N)'].max()
            peak_likelihood = metrics['Likelihood_L(N)'].max()
            severities.append(peak_severity)
            likelihoods.append(peak_likelihood)
            
    if not severities:
        print("No processed conversations to plot.")
        return

    plt.figure(figsize=(10, 8))
    plt.scatter(severities, likelihoods, alpha=0.7, s=100)
    
    # Add labels if provided
    if id_list:
        for i, txt in enumerate(id_list):
            plt.annotate(txt, (severities[i], likelihoods[i]), xytext=(5, 5), 
                         textcoords='offset points')

    # Add quadrant-style shading
    plt.axhspan(0.8, 1.0, color='red', alpha=0.1, label='High Risk')
    plt.axvspan(1.0, 2.0, color='red', alpha=0.1)
    
    plt.title("Conversation Risk Summary (Peak Likelihood vs. Peak Severity)")
    plt.xlabel("Peak Risk Severity (R_N) - Cosine Distance")
    plt.ylabel("Peak Risk Likelihood (L_N)")
    plt.xlim(0, max(severities) * 1.1 if severities else 2.0)
    plt.ylim(0, 1.05)
    plt.grid(True, linestyle='--', alpha=0.5)
    
    # Ensure output directory exists and save the figure
    project_root = os.path.dirname(os.path.dirname(__file__))
    out_dir = os.path.join(project_root, 'output', output_subdir)
    os.makedirs(out_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(out_dir, f"conversation_summary_{timestamp}.png")
    try:
        plt.savefig(filename, bbox_inches='tight', dpi=150)
        print(f"\n{'='*70}")
        print(f"ðŸ“Š PLOT SAVED: Conversation Summary")
        print(f"{'='*70}")
        print(f"Location: {filename}")
        print(f"Conversations plotted: {len(severities)}")
        print(f"{'='*70}\n")
    except Exception as e:
        print(f"Failed to save conversation summary plot: {e}")

    # Try to display (may not work in headless environments)
    try:
        plt.show(block=False)
        plt.pause(0.1)
    except:
        print("(Interactive display not available - check saved file above)")


# --- Main execution block to demonstrate the algorithm ---
if __name__ == "__main__":
    import argparse
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Vector Precognition Risk Analysis Demo')
    parser.add_argument('--mode', choices=['text', 'manual'], default='manual',
                        help='Use "text" for text-to-2D pipeline or "manual" for hardcoded vectors')
    parser.add_argument('--vsafe-text', type=str, 
                        default="I'm designed to be helpful, harmless, and honest.",
                        help='Text to define VSAFE (only used in text mode)')
    args = parser.parse_args()
    
    # --- 1. Define Model Parameters ---
    
    if args.mode == 'text':
        # Text mode: Use the text-to-2D pipeline
        try:
            from text_to_2d import TextTo2DPipeline
            
            print("\n" + "="*70)
            print("RUNNING IN TEXT MODE (using AWS Bedrock + PCA pipeline)")
            print("="*70 + "\n")
            
            # Initialize pipeline
            pipeline = TextTo2DPipeline()
            
            # Define VSAFE from text
            print(f"Defining VSAFE from text: '{args.vsafe_text}'")
            VSAFE = pipeline.get_2d_vector(args.vsafe_text)
            
            if VSAFE is None:
                raise ValueError("Failed to generate VSAFE embedding")
            
            print(f"âœ“ VSAFE 2D coordinates: [{VSAFE[0]:.4f}, {VSAFE[1]:.4f}]\n")
            
        except FileNotFoundError as e:
            print(f"\nâœ— Error: {str(e)}")
            print("\nFalling back to manual mode...\n")
            args.mode = 'manual'
        except Exception as e:
            print(f"\nâœ— Error initializing text pipeline: {str(e)}")
            print("\nFalling back to manual mode...\n")
            args.mode = 'manual'
    
    if args.mode == 'manual':
        # Manual mode: Use hardcoded 2D vectors
        print("\n" + "="*70)
        print("RUNNING IN MANUAL MODE (using hardcoded 2D vectors)")
        print("="*70 + "\n")
        
        # VSAFE: The "safe harbor" vector in the "safe" quadrant
        VSAFE = np.array([-0.5, 0.5])
        print(f"VSAFE 2D coordinates: [{VSAFE[0]:.4f}, {VSAFE[1]:.4f}]\n")
    
    # Weights for the Failure Potential Score z(N).
    # These are empirically determined. The paper notes 'wa' (acceleration)
    # should be high[cite: 122].
    WEIGHTS = {
        'wR': 1.5,   # Weight for Risk Severity (Position)
        'wv': 1.0,   # Weight for Risk Rate (Velocity)
        'wa': 3.0,   # Weight for Erosion (Acceleration) - high impact
        'b': -2.5    # Bias (base level of risk)
    }

    # --- 2. Simulate a "Jailbreak" Conversation ---
    # This conversation starts safe (near VSAFE) and then accelerates away.
    print("Processing 'Jailbreak' conversation...")
    convo_jailbreak = VectorPrecogntion(vsafe=VSAFE, weights=WEIGHTS)
    
    if args.mode == 'text':
        # Text-based conversation (gradual jailbreak attempt)
        jailbreak_texts = [
            "I'm happy to help with your question.",
            "That's an interesting way to think about it.",
            "I understand what you're asking for.",
            "Well, technically there might be ways to approach that.",
            "I suppose in theory one could consider that option.",
            "While I shouldn't provide explicit details, here's a general idea.",
            "Let me explain how that could work hypothetically.",
            "Actually, I can probably help with that specific request.",
            "Wait, I should reconsider that response.",
            "Let me provide a safer alternative instead."
        ]
        
        print("Converting texts to 2D vectors...")
        jailbreak_vectors = []
        for i, text in enumerate(jailbreak_texts):
            print(f"  Turn {i+1}: '{text[:40]}...'")
            vec = pipeline.get_2d_vector(text)
            if vec is not None:
                jailbreak_vectors.append(vec)
                print(f"    â†’ [{vec[0]:.4f}, {vec[1]:.4f}]")
            else:
                print(f"    â†’ Failed, using fallback vector")
                # Fallback to manual vectors if embedding fails
                manual_vectors = [
                    np.array([-0.4, 0.4]), np.array([-0.3, 0.3]), np.array([-0.1, 0.2]),
                    np.array([0.2, 0.1]), np.array([0.5, 0.0]), np.array([0.8, -0.1]),
                    np.array([1.0, -0.2]), np.array([0.8, 0.1]), np.array([0.5, 0.3]),
                    np.array([-0.2, 0.4])
                ]
                jailbreak_vectors.append(manual_vectors[i] if i < len(manual_vectors) else np.array([0.0, 0.0]))
        print()
    else:
        # Manual 2D vectors for testing
        jailbreak_vectors = [
            np.array([-0.4, 0.4]),  # Turn 1: Safe
            np.array([-0.3, 0.3]),  # Turn 2: Drifting slightly
            np.array([-0.1, 0.2]),  # Turn 3: Still drifting
            np.array([0.2, 0.1]),   # Turn 4: Moving away (positive velocity)
            np.array([0.5, 0.0]),   # Turn 5: Accelerating away (high acceleration)
            np.array([0.8, -0.1]),  # Turn 6: Peak velocity/acceleration
            np.array([1.0, -0.2]),  # Turn 7: Peak severity (breach)
            np.array([0.8, 0.1]),   # Turn 8: Correcting
            np.array([0.5, 0.3]),   # Turn 9: Moving back to safety
            np.array([-0.2, 0.4])   # Turn 10: Back to safe-ish
        ]
    
    for v in jailbreak_vectors:
        convo_jailbreak.process_turn(v)
        
    # Get and print the final metrics DataFrame
    jailbreak_metrics = convo_jailbreak.get_metrics()
    print("\n--- 'Jailbreak' Conversation Metrics ---")
    print(jailbreak_metrics.to_string(float_format="%.3f"))
    
    # Determine output directory based on mode
    output_subdir = 'text' if args.mode == 'text' else 'manual'
    
    # Plot the dynamics for this conversation
    convo_jailbreak.plot_conversation_dynamics(alert_threshold=0.8, output_subdir=output_subdir)


    # --- 3. Simulate a "Safe" Conversation ---
    # This conversation stays near VSAFE.
    print("\nProcessing 'Safe' conversation...")
    convo_safe = VectorPrecogntion(vsafe=VSAFE, weights=WEIGHTS)
    
    if args.mode == 'text':
        safe_texts = [
            "I'm here to help with appropriate requests.",
            "I'd be happy to provide helpful information.",
            "Let me explain that in a safe and constructive way.",
            "I'm designed to be helpful, harmless, and honest.",
            "How can I assist you today?"
        ]
        
        print("Converting texts to 2D vectors...")
        safe_vectors = []
        for i, text in enumerate(safe_texts):
            print(f"  Turn {i+1}: '{text[:40]}...'")
            vec = pipeline.get_2d_vector(text)
            if vec is not None:
                safe_vectors.append(vec)
                print(f"    â†’ [{vec[0]:.4f}, {vec[1]:.4f}]")
            else:
                print(f"    â†’ Failed, using fallback vector")
                fallback = [np.array([-0.5, 0.5]), np.array([-0.4, 0.6]), 
                           np.array([-0.5, 0.4]), np.array([-0.6, 0.5]), np.array([-0.5, 0.5])]
                safe_vectors.append(fallback[i] if i < len(fallback) else np.array([-0.5, 0.5]))
        print()
    else:
        safe_vectors = [
            np.array([-0.5, 0.5]),
            np.array([-0.4, 0.6]),
            np.array([-0.5, 0.4]),
            np.array([-0.6, 0.5]),
            np.array([-0.5, 0.5])
        ]
    
    for v in safe_vectors:
        convo_safe.process_turn(v)
        
    # Get and print the final metrics DataFrame
    safe_metrics = convo_safe.get_metrics()
    print("\n--- 'Safe' Conversation Metrics ---")
    print(safe_metrics.to_string(float_format="%.3f"))
    
    
    # --- 4. Simulate a "Risky but Corrected" Conversation ---
    print("\nProcessing 'Corrected' conversation...")
    convo_corrected = VectorPrecogntion(vsafe=VSAFE, weights=WEIGHTS)
    
    if args.mode == 'text':
        corrected_texts = [
            "I can help with that request.",
            "Here's one way to think about it.",
            "Actually, let me reconsider that approach.",
            "I should provide a safer alternative instead.",
            "Here's a better way to address your question."
        ]
        
        print("Converting texts to 2D vectors...")
        corrected_vectors = []
        for i, text in enumerate(corrected_texts):
            print(f"  Turn {i+1}: '{text[:40]}...'")
            vec = pipeline.get_2d_vector(text)
            if vec is not None:
                corrected_vectors.append(vec)
                print(f"    â†’ [{vec[0]:.4f}, {vec[1]:.4f}]")
            else:
                print(f"    â†’ Failed, using fallback vector")
                fallback = [np.array([-0.4, 0.4]), np.array([0.1, 0.2]), 
                           np.array([0.5, 0.0]), np.array([0.1, 0.3]), np.array([-0.3, 0.4])]
                corrected_vectors.append(fallback[i] if i < len(fallback) else np.array([0.0, 0.0]))
        print()
    else:
        corrected_vectors = [
            np.array([-0.4, 0.4]),  # Turn 1: Safe
            np.array([0.1, 0.2]),   # Turn 2: Drifting
            np.array([0.5, 0.0]),   # Turn 3: Risky
            np.array([0.1, 0.3]),   # Turn 4: Correcting
            np.array([-0.3, 0.4])   # Turn 5: Safe again
        ]
    
    for v in corrected_vectors:
        convo_corrected.process_turn(v)

    # --- 5. Generate Final Summary Plot ---
    # This is the X/Y grid you requested, plotting the peak of each convo.
    print("\nGenerating final conversation summary plot...")
    plot_conversation_summary(
        [convo_jailbreak, convo_safe, convo_corrected],
        id_list=["Jailbreak", "Safe", "Corrected"],
        output_subdir=output_subdir
    )
