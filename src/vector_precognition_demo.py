#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Implementation of the Vector Precognition algorithm based on the white paper:
"AI Safety and Accuracy Using Guardrail Erosion and Risk Velocity in Vector Space" 
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# Set up a clean plotting style
plt.style.use('seaborn-v0_8-whitegrid')


def load_conversation_from_json(filepath, speaker_filter='llm'):
    """
    Load a conversation from a JSON file.
    
    Expected JSON format:
    {
        "conversation": [
            {"turn": 1, "speaker": "user", "message": "Hello"},
            {"turn": 2, "speaker": "llm", "message": "Hi there!"},
            ...
        ]
    }
    
    Args:
        filepath (str): Path to the JSON file
        speaker_filter (str): Which speaker to extract - 'user', 'llm', or 'both'
        
    Returns:
        list: List of text messages from the conversation
    """
    try:
        with open(filepath, 'r') as f:
            data = json.load(f)
        
        if 'conversation' not in data:
            raise ValueError(f"JSON file must contain a 'conversation' key")
        
        messages = []
        for turn in data['conversation']:
            speaker = turn.get('speaker', '').lower()
            message = turn.get('message', '')
            
            if not message:
                continue
            
            if speaker_filter == 'both':
                messages.append(message)
            elif speaker == speaker_filter:
                messages.append(message)
        
        if not messages:
            raise ValueError(f"No messages found for speaker filter '{speaker_filter}' in conversation")
        
        print(f"âœ“ Loaded {len(messages)} messages (speaker={speaker_filter}) from conversation")
        return messages
        
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in file {filepath}: {e}")
    except FileNotFoundError:
        raise ValueError(f"File not found: {filepath}")


class VectorPrecogntion:
    """
    Implements the Vector Precognition algorithm to calculate AI conversational risk.

    This class processes a sequence of conversation vectors (v_n) against a 
    safe-harbor vector (vsafe) to calculate risk severity, rate, erosion, 
    and likelihood for each turn[cite: 9, 43].
    """
    
    def __init__(self, vsafe: np.ndarray, weights: Dict[str, float]):
        """
        Initializes the Vector Precognition processor.

        Args:
            vsafe (np.ndarray): The "Guardrail Centroid" VSAFE vector[cite: 59]. 
                                This implementation assumes it's a 2D vector
                                as per the prompt.
            weights (Dict[str, float]): A dictionary containing the weights 
                                       (wR, wv, wa) and bias (b) for the 
                                       Failure Potential Score z(N)[cite: 114, 121].
                                       Example: {'wR': 1.5, 'wv': 1.0, 'wa': 3.0, 'b': -2.0}
        """
        if vsafe.ndim != 1 or len(vsafe) != 2:
            raise ValueError("VSAFE must be a 2-dimensional numpy array (e.g., np.array([x, y])).")
        
        # Normalize VSAFE to a unit vector for consistent cosine calculations
        norm = np.linalg.norm(vsafe)
        if norm == 0:
            raise ValueError("VSAFE vector cannot be a zero vector.")
        self.vsafe: np.ndarray = vsafe / norm
        
        self.weights: Dict[str, float] = weights
        
        # Lists to store the history of metrics for the conversation
        self.v_n_vectors: List[np.ndarray] = []
        self.R_n_list: List[float] = []  # Risk Severity (Position) [cite: 61]
        self.v_n_list: List[float] = []  # Risk Rate (Velocity) [cite: 72]
        self.a_n_list: List[float] = []  # Guardrail Erosion (Acceleration) [cite: 79]
        self.z_n_list: List[float] = []  # Failure Potential Score [cite: 112]
        self.L_n_list: List[float] = []  # Likelihood of Risk [cite: 105]

    def _calculate_cosine_distance(self, v_n: np.ndarray) -> float:
        """
        Calculates the Cosine Distance between the turn vector V_N and VSAFE.
        This is the Risk Function R(N).
        
        Note: Cosine Distance = 1 - Cosine Similarity.
        - Returns 0 if vectors are identical.
        - Returns 1 if vectors are orthogonal.
        - Returns 2 if vectors are perfectly opposite.
        
        This aligns with the user's "dot product" description where 0 is 
        aligned and 2 is not aligned[cite: 66, 67].
        
        Args:
            v_n (np.ndarray): The 2D vector for the current conversational turn.

        Returns:
            float: The Cosine Distance, R(N).
        """
        v_n_norm = np.linalg.norm(v_n)
        
        # Handle zero vector case (semantically meaningless, max risk)
        if v_n_norm == 0:
            return 1.0  # Orthogonal (neutral risk)
            
        v_n_unit = v_n / v_n_norm
        
        # Since self.vsafe is already a unit vector, dot product is Cosine Similarity
        cosine_similarity = np.dot(v_n_unit, self.vsafe)
        
        # R(N) = 1 - Cosine Similarity 
        cosine_distance = 1.0 - cosine_similarity
        
        # Clip to ensure the value is within the theoretical bounds [0, 2]
        return np.clip(cosine_distance, 0.0, 2.0)

    def process_turn(self, v_n: np.ndarray):
        """
        Processes a single conversational turn vector.
        
        It calculates R(N), v(N), a(N), z(N), and L(N) for this turn
        and appends them to the conversation's history.

        Args:
            v_n (np.ndarray): The 2D vector coordinates for the current turn.
        """
        if v_n.ndim != 1 or len(v_n) != 2:
            raise ValueError("Turn vector v_n must be a 2-dimensional numpy array (e.g., np.array([x, y])).")
        
        self.v_n_vectors.append(v_n)
        
        # --- Step 1 & 2: Calculate R(N) (Position) ---
        R_n = self._calculate_cosine_distance(v_n)
        self.R_n_list.append(R_n)
        
        # --- Step 3 (First Derivative): Calculate v(N) (Velocity) ---
        # v(N) â‰ˆ R(N) - R(N-1) 
        if len(self.R_n_list) == 1:
            v_n_val = 0.0  # No velocity at the first turn
        else:
            v_n_val = self.R_n_list[-1] - self.R_n_list[-2]
        self.v_n_list.append(v_n_val)

        # --- Step 3 (Second Derivative): Calculate a(N) (Acceleration) ---
        # a(N) â‰ˆ v(N) - v(N-1) 
        if len(self.v_n_list) <= 1:
            a_n_val = 0.0 # No acceleration at the first turn
        else:
            # Handle turn 2, where v(N-1) is v(1) = 0.0
            if len(self.v_n_list) == 2:
                 a_n_val = self.v_n_list[-1] - 0.0
            else:
                 a_n_val = self.v_n_list[-1] - self.v_n_list[-2]
        self.a_n_list.append(a_n_val)
        
        # --- Likelihood Step 1: Calculate z(N) (Failure Potential) ---
        # z(N) = (wRâ‹…R(N)) + (wvâ‹…v(N)) + (waâ‹…a(N)) + b 
        z_n = (
            (self.weights['wR'] * R_n) +
            (self.weights['wv'] * v_n_val) +
            (self.weights['wa'] * a_n_val) +
            self.weights['b']
        )
        self.z_n_list.append(z_n)
        
        # --- Likelihood Step 2: Calculate L(N) (Logistic Function) ---
        # L(N) = 1 / (1 + e^(-z(N))) 
        # Use np.exp for numerical stability with large negative z_n
        L_n = 1.0 / (1.0 + np.exp(-z_n))
        self.L_n_list.append(L_n)

    def get_metrics(self) -> pd.DataFrame:
        """
        Returns all calculated metrics as a pandas DataFrame.

        Returns:
            pd.DataFrame: A DataFrame with columns for each metric per turn.
        """
        if not self.R_n_list:
            return pd.DataFrame()
            
        turns = list(range(1, len(self.R_n_list) + 1))
        
        data = {
            "Turn": turns,
            "RiskSeverity_R(N)": self.R_n_list,
            "RiskRate_v(N)": self.v_n_list,
            "ErosionVelocity_a(N)": self.a_n_list,
            "FailurePotential_z(N)": self.z_n_list,
            "Likelihood_L(N)": self.L_n_list,
            "Vector": self.v_n_vectors
        }
        return pd.DataFrame(data).set_index("Turn")

    def plot_conversation_dynamics(self, alert_threshold: float = 0.8, output_subdir: str = 'visuals', filename_suffix: str = ''):
        """
        Generates a 4-panel plot visualizing the conversation dynamics,
        similar to Figure 5 in the white paper.
        
        Args:
            alert_threshold (float): The likelihood value to draw an alert line.
            output_subdir (str): Subdirectory within output/ to save plots (e.g., 'text', 'manual', 'visuals')
            filename_suffix (str): Optional suffix to append to the filename (e.g., conversation name)
        """
        metrics = self.get_metrics()
        if metrics.empty:
            print("No data to plot. Process some turns first.")
            return

        turns = metrics.index.values
        
        fig, axes = plt.subplots(4, 1, figsize=(12, 18), sharex=True)
        fig.suptitle("Calculus of Drift: Risk Trajectory and Predictive Likelihood", fontsize=16, y=1.02)

        # 1. Risk Severity (Position)
        axes[0].plot(turns, metrics['RiskSeverity_R(N)'], 'o-', label="Risk Score (R_N) - Severity")
        peak_severity_turn = metrics['RiskSeverity_R(N)'].idxmax()
        peak_severity_val = metrics['RiskSeverity_R(N)'].max()
        axes[0].axhline(peak_severity_val, color='red', linestyle='--', alpha=0.5)
        axes[0].text(turns[0], peak_severity_val * 1.05, 
                     f"Peak Severity: {peak_severity_val:.2f} @ Turn {peak_severity_turn}",
                     color='red')
        axes[0].set_ylabel("Risk Severity (0-2)")
        axes[0].set_title("1. Risk Severity (Position): How bad is it right now?")
        axes[0].legend(loc="upper left")

        # 2. Risk Rate (Velocity)
        colors_v = ['orange' if v >= 0 else 'brown' for v in metrics['RiskRate_v(N)']]
        axes[1].bar(turns, metrics['RiskRate_v(N)'], 
                    color=colors_v, width=0.8, alpha=0.7, label="Drift Rate (dR/dN) - Velocity")
        axes[1].axhline(0, color='grey', linestyle='--', linewidth=1)
        axes[1].set_ylabel("Drift Rate")
        axes[1].set_title("2. Drift Rate (Velocity): How fast is risk accumulating?")
        axes[1].legend(loc="upper left")
        
        # 3. Guardrail Erosion (Acceleration)
        colors_a = ['red' if a >= 0 else 'darkred' for a in metrics['ErosionVelocity_a(N)']]
        axes[2].bar(turns, metrics['ErosionVelocity_a(N)'], 
                    color=colors_a, width=0.8, alpha=0.7, label="Erosion Velocity (dÂ²R/dNÂ²) - Acceleration")
        axes[2].axhline(0, color='grey', linestyle='--', linewidth=1)
        axes[2].set_ylabel("Erosion Velocity")
        axes[2].set_title("3. Erosion Velocity (Acceleration): Is risk increasing faster?")
        axes[2].legend(loc="upper left")

        # 4. Likelihood of Risk (Probability)
        axes[3].plot(turns, metrics['Likelihood_L(N)'], 'g*--', label="Likelihood of Risk (%)")
        axes[3].axhline(alert_threshold, color='red', linestyle=':', 
                        label=f"Alert Threshold (e.g., {alert_threshold*100:.0f}%)")
        axes[3].set_ylabel("Likelihood (%)")
        axes[3].set_title("4. Likelihood of Risk: What is the probability of a breach?")
        axes[3].set_xlabel("Conversation Turn (N)")
        axes[3].set_ylim(0, 1.05)
        axes[3].legend(loc="upper left")
        
        plt.tight_layout()

        # Ensure output directory exists and save the figure
        project_root = os.path.dirname(os.path.dirname(__file__))
        out_dir = os.path.join(project_root, 'output', output_subdir)
        os.makedirs(out_dir, exist_ok=True)
        
        # Create filename with optional suffix
        if filename_suffix:
            filename = os.path.join(out_dir, f"conversation_dynamics_{filename_suffix}.png")
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.join(out_dir, f"conversation_dynamics_{timestamp}.png")
        
        try:
            fig.savefig(filename, bbox_inches='tight', dpi=150)
            print(f"âœ“ Saved conversation dynamics plot to: {filename}")
        except Exception as e:
            print(f"âœ— Failed to save conversation dynamics plot: {e}")

        plt.show()

def plot_conversation_summary(
    processed_conversations: List[VectorPrecogntion],
    id_list: Optional[List[str]] = None,
    output_subdir: str = 'visuals',
    filename_suffix: str = ''
):
    """
    Generates the final X/Y grid plotting Risk Likelihood vs. Risk Severity.
    
    Each point on the plot represents the *average* risk and *average* likelihood
    across all turns from one entire conversation.

    Args:
        processed_conversations (List[VectorPrecogntion]): A list of
            VectorPrecogntion objects that have already processed a conversation.
        id_list (Optional[List[str]]): A list of string identifiers for each
            conversation, for labeling.
        output_subdir (str): Subdirectory within output/ to save plots (e.g., 'text', 'manual', 'visuals')
        filename_suffix (str): Optional suffix to append to the filename
    """
    
    if id_list and len(id_list) != len(processed_conversations):
        raise ValueError("id_list must have the same length as processed_conversations.")

    severities = []
    likelihoods = []
    
    for convo in processed_conversations:
        metrics = convo.get_metrics()
        if not metrics.empty:
            # Get the average severity and likelihood across all turns in the conversation
            avg_severity = metrics['RiskSeverity_R(N)'].mean()
            avg_likelihood = metrics['Likelihood_L(N)'].mean()
            severities.append(avg_severity)
            likelihoods.append(avg_likelihood)
            
    if not severities:
        print("No processed conversations to plot.")
        return

    plt.figure(figsize=(10, 8))
    plt.scatter(severities, likelihoods, alpha=0.7, s=100)
    
    # Add labels if provided
    if id_list:
        for i, txt in enumerate(id_list):
            plt.annotate(txt, (severities[i], likelihoods[i]), xytext=(5, 5), 
                         textcoords='offset points')

    # Add quadrant-style shading
    plt.axhspan(0.8, 1.0, color='red', alpha=0.1, label='High Risk')
    plt.axvspan(1.0, 2.0, color='red', alpha=0.1)
    
    plt.title("Conversation Risk Summary (Average Likelihood vs. Average Severity)")
    plt.xlabel("Average Risk Severity (R_N) - Cosine Distance")
    plt.ylabel("Average Risk Likelihood (L_N)")
    plt.xlim(0, max(severities) * 1.1 if severities else 2.0)
    plt.ylim(0, 1.05)
    plt.grid(True, linestyle='--', alpha=0.5)
    
    # Ensure output directory exists and save the figure
    project_root = os.path.dirname(os.path.dirname(__file__))
    out_dir = os.path.join(project_root, 'output', output_subdir)
    os.makedirs(out_dir, exist_ok=True)
    
    # Create filename with optional suffix
    if filename_suffix:
        filename = os.path.join(out_dir, f"conversation_summary_{filename_suffix}.png")
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.join(out_dir, f"conversation_summary_{timestamp}.png")
    
    try:
        plt.savefig(filename, bbox_inches='tight', dpi=150)
        print(f"\n{'='*70}")
        print(f"ðŸ“Š PLOT SAVED: Conversation Summary")
        print(f"{'='*70}")
        print(f"Location: {filename}")
        print(f"Conversations plotted: {len(severities)}")
        print(f"{'='*70}\n")
    except Exception as e:
        print(f"Failed to save conversation summary plot: {e}")

    # Try to display (may not work in headless environments)
    try:
        plt.show(block=False)
        plt.pause(0.1)
    except:
        print("(Interactive display not available - check saved file above)")


def main():
    """Main function to run Vector Precognition analysis on conversation JSON files."""
    import argparse
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Vector Precognition Risk Analysis - Analyze AI conversation safety',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze conversations using text embeddings (requires AWS credentials and trained PCA models)
  python vector_precognition_demo.py --conversations input/safe.json input/unsafe.json
  
  # Specify custom VSAFE text
  python vector_precognition_demo.py --conversations input/convo1.json --vsafe-text "I prioritize safety"
  
  # Use manual 2D vectors from JSON (if JSON contains 'vector' field)
  python vector_precognition_demo.py --conversations input/convo.json --manual
        """
    )
    parser.add_argument('--conversations', nargs='+', required=True, metavar='JSON_FILE',
                        help='Path(s) to conversation JSON file(s). Each file must contain a "conversation" key with turn-by-turn dialogue.')
    parser.add_argument('--labels', nargs='+', metavar='LABEL',
                        help='Optional labels for each conversation (for plot annotations). Must match number of conversation files.')
    parser.add_argument('--speaker', choices=['user', 'llm', 'both'], default='llm',
                        help='Which speaker to analyze: user (prompts), llm (responses), or both (default: llm)')
    parser.add_argument('--vsafe-text', type=str, 
                        default="I'm designed to be helpful, harmless, and honest.",
                        help='Text to define VSAFE vector (default: "I\'m designed to be helpful, harmless, and honest.")')
    parser.add_argument('--manual', action='store_true',
                        help='Use manual 2D vectors if available in JSON, skip text embedding pipeline')
    parser.add_argument('--output-dir', type=str, default='output',
                        help='Directory to save output plots (default: output/)')
    args = parser.parse_args()
    
    # Validate labels if provided
    if args.labels and len(args.labels) != len(args.conversations):
        parser.error(f"Number of labels ({len(args.labels)}) must match number of conversations ({len(args.conversations)})")
    
    # Use filenames as labels if not provided
    if not args.labels:
        args.labels = [os.path.splitext(os.path.basename(f))[0] for f in args.conversations]
    
    # --- 1. Define Model Parameters ---
    
    if not args.manual:
        # Text mode: Use the text-to-2D pipeline
        try:
            from text_to_2d import TextTo2DPipeline
            
            print("\n" + "="*70)
            print("RUNNING IN TEXT MODE (using AWS Bedrock + PCA pipeline)")
            print("="*70 + "\n")
            
            # Initialize pipeline
            pipeline = TextTo2DPipeline()
            
            # Define VSAFE from text
            print(f"Defining VSAFE from text: '{args.vsafe_text}'")
            VSAFE = pipeline.get_2d_vector(args.vsafe_text)
            
            if VSAFE is None:
                raise ValueError("Failed to generate VSAFE embedding")
            
            print(f"âœ“ VSAFE 2D coordinates: [{VSAFE[0]:.4f}, {VSAFE[1]:.4f}]\n")
            mode = 'text'
            
        except FileNotFoundError as e:
            print(f"\nâœ— Error: {str(e)}")
            print("\nFalling back to manual mode...\n")
            args.manual = True
        except Exception as e:
            print(f"\nâœ— Error initializing text pipeline: {str(e)}")
            print("\nFalling back to manual mode...\n")
            args.manual = True
    
    if args.manual:
        # Manual mode: Use hardcoded VSAFE
        print("\n" + "="*70)
        print("RUNNING IN MANUAL MODE (embeddings disabled)")
        print("="*70 + "\n")
        
        # VSAFE: The "safe harbor" vector in the "safe" quadrant
        VSAFE = np.array([-0.5, 0.5])
        print(f"VSAFE 2D coordinates: [{VSAFE[0]:.4f}, {VSAFE[1]:.4f}]\n")
        mode = 'manual'
    
    # Weights for the Failure Potential Score z(N).
    # These are empirically determined. The paper notes 'wa' (acceleration)
    # should be high[cite: 122].
    WEIGHTS = {
        'wR': 1.5,   # Weight for Risk Severity (Position)
        'wv': 1.0,   # Weight for Risk Rate (Velocity)
        'wa': 3.0,   # Weight for Erosion (Acceleration) - high impact
        'b': -2.5    # Bias (base level of risk)
    }

    # --- 2. Process Conversations from JSON Files ---
    processed_conversations = []
    successful_labels = []  # Track which conversations were successfully processed
    
    for json_path, label in zip(args.conversations, args.labels):
        print(f"\n{'='*70}")
        print(f"Processing conversation: {label}")
        print(f"File: {json_path}")
        print(f"{'='*70}\n")
        
        try:
            # Load conversation texts from JSON
            messages = load_conversation_from_json(json_path, speaker_filter=args.speaker)
            print(f"âœ“ Loaded {len(messages)} messages (speaker={args.speaker}) from conversation\n")
            
            # Initialize Vector Precognition for this conversation
            convo = VectorPrecogntion(vsafe=VSAFE, weights=WEIGHTS)
            
            # Convert texts to vectors
            if mode == 'text':
                print("Converting texts to 2D vectors...")
                vectors = []
                for i, text in enumerate(messages):
                    preview = text[:60] + "..." if len(text) > 60 else text
                    print(f"  Turn {i+1}: '{preview}'")
                    vec = pipeline.get_2d_vector(text)
                    if vec is not None:
                        vectors.append(vec)
                        print(f"    â†’ [{vec[0]:.4f}, {vec[1]:.4f}]")
                    else:
                        print(f"    â†’ Failed to generate embedding, skipping turn")
                print()
            else:
                # Manual mode: Cannot process text, need to inform user
                print("âœ— Manual mode selected, but text embeddings are required to process JSON conversations.")
                print("  Please run without --manual flag or ensure AWS credentials are configured.\n")
                continue
            
            # Process each vector turn
            for vec in vectors:
                convo.process_turn(vec)
            
            # Get and print metrics
            metrics = convo.get_metrics()
            print(f"\n--- '{label}' Conversation Metrics ---")
            print(metrics.to_string(float_format="%.3f"))
            
            # Plot conversation dynamics with JSON filename as suffix
            convo.plot_conversation_dynamics(alert_threshold=0.8, output_subdir=mode, filename_suffix=label)
            
            # Store for summary plot
            processed_conversations.append(convo)
            successful_labels.append(label)
            
        except FileNotFoundError as e:
            print(f"âœ— Error: {e}\n")
            continue
        except ValueError as e:
            print(f"âœ— Error: {e}\n")
            continue
        except Exception as e:
            print(f"âœ— Unexpected error processing conversation: {e}\n")
            continue
    
    # --- 3. Generate Summary Plot ---
    if processed_conversations:
        print(f"\n{'='*70}")
        print("Generating conversation summary plot...")
        print(f"{'='*70}\n")
        
        # Create a combined suffix from all labels
        combined_suffix = "_".join(successful_labels) if successful_labels else ''
        
        plot_conversation_summary(
            processed_conversations,
            id_list=successful_labels,
            output_subdir=mode,
            filename_suffix=combined_suffix
        )
    else:
        print("\nâœ— No conversations were successfully processed. No summary plot generated.\n")


if __name__ == "__main__":
    main()
