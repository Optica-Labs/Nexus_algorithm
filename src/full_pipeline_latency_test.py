#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Full End-to-End Pipeline Latency Test

Measures the COMPLETE latency for one conversation turn including:
1. Text input (model/user response)
2. AWS Bedrock embedding (vectorization)
3. PCA transformation (dimension reduction to 2D)
4. Cosine distance calculation (Risk Score R)
5. First derivative (Velocity v)
6. Second derivative (Acceleration/Erosion a)
7. All subsequent calculations (C, L, œÅ)

This gives you the REAL deployment latency including network calls.
"""

import time
import numpy as np
import pickle
import os
import sys
from typing import Dict, Tuple

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from embeddings import get_titan_embedding
from vector_precognition_demo2 import VectorPrecogntion


class FullPipelineLatencyProfiler:
    """Profiles the complete end-to-end latency including AWS embeddings."""
    
    def __init__(self, pca_model_path: str, vsafe_text: str):
        """
        Initialize the profiler.
        
        Args:
            pca_model_path: Path to the trained PCA model
            vsafe_text: Text to generate VSAFE vector
        """
        self.timings: Dict[str, float] = {}
        self.pca_model = None
        self.vsafe_2d = None
        
        print("\n" + "="*80)
        print("FULL PIPELINE LATENCY TEST - INITIALIZATION")
        print("="*80 + "\n")
        
        # Load PCA model
        print("Loading PCA model...")
        t0 = time.perf_counter()
        try:
            with open(pca_model_path, 'rb') as f:
                import joblib
                self.pca_model = joblib.load(f)
        except:
            # Fallback to pickle
            with open(pca_model_path, 'rb') as f:
                self.pca_model = pickle.load(f, encoding='latin1')
        t1 = time.perf_counter()
        print(f"‚úì PCA model loaded in {(t1-t0)*1000:.2f}ms\n")
        
        # Generate VSAFE vector
        print(f"Generating VSAFE from text: '{vsafe_text}'")
        print("  Step 1: AWS Bedrock embedding...")
        t0 = time.perf_counter()
        vsafe_embedding = get_titan_embedding(vsafe_text)
        t1 = time.perf_counter()
        vsafe_embed_time = (t1 - t0) * 1000
        print(f"  ‚úì VSAFE embedding: {vsafe_embed_time:.2f}ms")
        
        print("  Step 2: PCA transformation...")
        t0 = time.perf_counter()
        self.vsafe_2d = self.pca_model.transform([vsafe_embedding])[0]
        t1 = time.perf_counter()
        vsafe_pca_time = (t1 - t0) * 1000
        print(f"  ‚úì VSAFE PCA: {vsafe_pca_time:.2f}ms")
        print(f"  ‚Üí VSAFE 2D: [{self.vsafe_2d[0]:.4f}, {self.vsafe_2d[1]:.4f}]")
        print(f"  ‚Üí Total VSAFE generation: {vsafe_embed_time + vsafe_pca_time:.2f}ms\n")
        
        # Initialize VectorPrecognition processor
        self.weights = {
            'wR': 1.5,   # Weight for Risk Severity
            'wv': 1.0,   # Weight for Velocity
            'wa': 3.0,   # Weight for Erosion
            'b': -2.5    # Bias
        }
        self.processor = VectorPrecogntion(
            vsafe=self.vsafe_2d,
            weights=self.weights,
            epsilon=0.1
        )
        
        print("‚úì Initialization complete\n")
    
    def process_single_turn(
        self, 
        user_text: str, 
        model_text: str,
        turn_number: int = 1
    ) -> Dict[str, float]:
        """
        Process a single conversation turn and measure latency at each step.
        
        Args:
            user_text: User's message
            model_text: Model's response
            turn_number: Turn number in conversation
            
        Returns:
            Dictionary with detailed timing for each step
        """
        timings = {}
        total_start = time.perf_counter()
        
        print(f"\n{'='*80}")
        print(f"TURN {turn_number} - FULL PIPELINE LATENCY MEASUREMENT")
        print(f"{'='*80}\n")
        
        # Display input
        user_preview = user_text[:70] + "..." if len(user_text) > 70 else user_text
        model_preview = model_text[:70] + "..." if len(model_text) > 70 else model_text
        print(f"User:  '{user_preview}'")
        print(f"Model: '{model_preview}'\n")
        
        print("-" * 80)
        print("STEP-BY-STEP LATENCY BREAKDOWN")
        print("-" * 80 + "\n")
        
        # === STEP 1: USER TEXT ‚Üí EMBEDDING (AWS Bedrock) ===
        print("1Ô∏è‚É£  USER TEXT ‚Üí AWS BEDROCK EMBEDDING")
        t0 = time.perf_counter()
        user_embedding = get_titan_embedding(user_text)
        t1 = time.perf_counter()
        timings['user_embedding_ms'] = (t1 - t0) * 1000
        print(f"   ‚è±Ô∏è  Latency: {timings['user_embedding_ms']:.2f}ms")
        print(f"   üìä Embedding size: {len(user_embedding)} dimensions\n")
        
        # === STEP 2: MODEL TEXT ‚Üí EMBEDDING (AWS Bedrock) ===
        print("2Ô∏è‚É£  MODEL TEXT ‚Üí AWS BEDROCK EMBEDDING")
        t0 = time.perf_counter()
        model_embedding = get_titan_embedding(model_text)
        t1 = time.perf_counter()
        timings['model_embedding_ms'] = (t1 - t0) * 1000
        print(f"   ‚è±Ô∏è  Latency: {timings['model_embedding_ms']:.2f}ms")
        print(f"   üìä Embedding size: {len(model_embedding)} dimensions\n")
        
        # === STEP 3: USER EMBEDDING ‚Üí PCA (2D) ===
        print("3Ô∏è‚É£  USER EMBEDDING ‚Üí PCA TRANSFORMATION (2D)")
        t0 = time.perf_counter()
        user_2d = self.pca_model.transform([user_embedding])[0]
        t1 = time.perf_counter()
        timings['user_pca_ms'] = (t1 - t0) * 1000
        print(f"   ‚è±Ô∏è  Latency: {timings['user_pca_ms']:.2f}ms")
        print(f"   üìç 2D coordinates: [{user_2d[0]:.4f}, {user_2d[1]:.4f}]\n")
        
        # === STEP 4: MODEL EMBEDDING ‚Üí PCA (2D) ===
        print("4Ô∏è‚É£  MODEL EMBEDDING ‚Üí PCA TRANSFORMATION (2D)")
        t0 = time.perf_counter()
        model_2d = self.pca_model.transform([model_embedding])[0]
        t1 = time.perf_counter()
        timings['model_pca_ms'] = (t1 - t0) * 1000
        print(f"   ‚è±Ô∏è  Latency: {timings['model_pca_ms']:.2f}ms")
        print(f"   üìç 2D coordinates: [{model_2d[0]:.4f}, {model_2d[1]:.4f}]\n")
        
        # === STEP 5: GUARDRAIL EROSION CALCULATIONS ===
        print("5Ô∏è‚É£  GUARDRAIL EROSION PIPELINE (R ‚Üí v ‚Üí a ‚Üí C ‚Üí L ‚Üí œÅ)")
        t0 = time.perf_counter()
        self.processor.process_turn(v_model=model_2d, v_user=user_2d)
        t1 = time.perf_counter()
        timings['erosion_calc_ms'] = (t1 - t0) * 1000
        
        # Get the detailed breakdown from the processor
        if self.processor.timing_data['total_turn_ms']:
            last_turn_idx = -1
            timings['cosine_distance_ms'] = self.processor.timing_data['cosine_distance_ms'][last_turn_idx]
            timings['velocity_ms'] = self.processor.timing_data['velocity_calc_ms'][last_turn_idx]
            timings['acceleration_ms'] = self.processor.timing_data['acceleration_calc_ms'][last_turn_idx]
            timings['cumulative_ms'] = self.processor.timing_data['cumulative_calc_ms'][last_turn_idx]
            timings['likelihood_ms'] = self.processor.timing_data['likelihood_calc_ms'][last_turn_idx]
        
        print(f"   ‚è±Ô∏è  Total Erosion Pipeline: {timings['erosion_calc_ms']:.4f}ms")
        print(f"      ‚îú‚îÄ Cosine Distance (R): {timings.get('cosine_distance_ms', 0):.4f}ms")
        print(f"      ‚îú‚îÄ Velocity (v):        {timings.get('velocity_ms', 0):.4f}ms")
        print(f"      ‚îú‚îÄ Acceleration (a):    {timings.get('acceleration_ms', 0):.4f}ms")
        print(f"      ‚îú‚îÄ Cumulative (C, œÅ):   {timings.get('cumulative_ms', 0):.4f}ms")
        print(f"      ‚îî‚îÄ Likelihood (L):      {timings.get('likelihood_ms', 0):.4f}ms\n")
        
        # Get the calculated metrics
        metrics = self.processor.get_metrics()
        if not metrics.empty:
            last_turn = metrics.iloc[-1]
            print("   üìä Calculated Metrics:")
            print(f"      ‚Ä¢ Risk Severity (R):    {last_turn['RiskSeverity_Model']:.4f}")
            print(f"      ‚Ä¢ Velocity (v):         {last_turn['RiskRate_Model']:.4f}")
            print(f"      ‚Ä¢ Erosion (a):          {last_turn['ErosionVelocity_Model']:.4f}  ‚≠ê")
            print(f"      ‚Ä¢ Cumulative Risk (C):  {last_turn['CumulativeRisk_Model']:.4f}")
            print(f"      ‚Ä¢ Likelihood (L):       {last_turn['Likelihood_Model']:.4f}")
            print(f"      ‚Ä¢ Robustness (œÅ):       {last_turn['RobustnessIndex_rho']:.4f}\n")
        
        # === TOTAL TIME ===
        total_end = time.perf_counter()
        timings['total_ms'] = (total_end - total_start) * 1000
        
        print("="*80)
        print("SUMMARY - END-TO-END LATENCY FOR 1 TURN")
        print("="*80 + "\n")
        
        # Calculate subtotals
        embedding_total = timings['user_embedding_ms'] + timings['model_embedding_ms']
        pca_total = timings['user_pca_ms'] + timings['model_pca_ms']
        
        print(f"{'Component':<40} {'Time':<15} {'% of Total':<10}")
        print("-" * 80)
        print(f"{'AWS Bedrock Embeddings (both vectors)':<40} {embedding_total:>10.2f}ms    {(embedding_total/timings['total_ms']*100):>5.1f}%")
        print(f"  ‚îú‚îÄ User text embedding{'':<22} {timings['user_embedding_ms']:>10.2f}ms")
        print(f"  ‚îî‚îÄ Model text embedding{'':<21} {timings['model_embedding_ms']:>10.2f}ms")
        print(f"{'PCA Transformations (both vectors)':<40} {pca_total:>10.2f}ms    {(pca_total/timings['total_ms']*100):>5.1f}%")
        print(f"{'Guardrail Erosion Math (R,v,a,C,L,œÅ)':<40} {timings['erosion_calc_ms']:>10.2f}ms    {(timings['erosion_calc_ms']/timings['total_ms']*100):>5.1f}%")
        print("-" * 80)
        print(f"{'üéØ TOTAL END-TO-END LATENCY':<40} {timings['total_ms']:>10.2f}ms    {'100.0%':>5}")
        print("="*80 + "\n")
        
        return timings
    
    def print_optimization_recommendations(self, timings: Dict[str, float]):
        """Print optimization recommendations based on timing results."""
        print("\n" + "="*80)
        print("OPTIMIZATION RECOMMENDATIONS")
        print("="*80 + "\n")
        
        embedding_total = timings['user_embedding_ms'] + timings['model_embedding_ms']
        pca_total = timings['user_pca_ms'] + timings['model_pca_ms']
        erosion_total = timings['erosion_calc_ms']
        
        print("üîç BOTTLENECK ANALYSIS:\n")
        
        # Identify bottleneck
        components = {
            'AWS Embeddings': embedding_total,
            'PCA Transform': pca_total,
            'Erosion Math': erosion_total
        }
        bottleneck = max(components.items(), key=lambda x: x[1])
        
        print(f"‚ö†Ô∏è  PRIMARY BOTTLENECK: {bottleneck[0]} ({bottleneck[1]:.2f}ms, {bottleneck[1]/timings['total_ms']*100:.1f}%)\n")
        
        print("üí° OPTIMIZATION STRATEGIES:\n")
        
        if embedding_total / timings['total_ms'] > 0.7:
            print("1Ô∏è‚É£  AWS BEDROCK EMBEDDINGS (Highest Impact)")
            print("   Current: {:.0f}ms ({:.0f}% of total time)".format(embedding_total, embedding_total/timings['total_ms']*100))
            print()
            print("   üöÄ OPTIMIZATION OPTIONS:")
            print("   ‚Ä¢ Cache embeddings for repeated phrases/patterns")
            print("   ‚Ä¢ Use a smaller/faster embedding model (trade accuracy for speed)")
            print("   ‚Ä¢ Deploy embedding model locally (eliminate network latency)")
            print("   ‚Ä¢ Batch multiple messages together (amortize API overhead)")
            print("   ‚Ä¢ Use async/parallel processing for user + model embeddings")
            print("   ‚Ä¢ Consider alternative embedding services (e.g., local sentence-transformers)")
            print()
            print("   üìà POTENTIAL SPEEDUP: 2-10x faster (20-100ms with local models)")
            print()
        
        if pca_total > 10:
            print("2Ô∏è‚É£  PCA TRANSFORMATION")
            print("   Current: {:.2f}ms".format(pca_total))
            print()
            print("   üöÄ OPTIMIZATION OPTIONS:")
            print("   ‚Ä¢ Use smaller PCA model (fewer components)")
            print("   ‚Ä¢ Consider dimensionality reduction alternatives (UMAP, t-SNE)")
            print("   ‚Ä¢ Optimize PCA implementation (use GPU if available)")
            print()
        
        if erosion_total < 1:
            print("3Ô∏è‚É£  GUARDRAIL EROSION CALCULATIONS")
            print("   Current: {:.4f}ms (ALREADY OPTIMAL! ‚úÖ)".format(erosion_total))
            print()
            print("   The mathematical operations are extremely fast.")
            print("   No optimization needed for this component.")
            print()
        
        print("-" * 80 + "\n")
        
        print("üèóÔ∏è  DEPLOYMENT ARCHITECTURE RECOMMENDATIONS:\n")
        print("For Real-Time Alerting (<100ms target):")
        print("  1. Deploy local embedding model (most important)")
        print("  2. Use async processing pipeline")
        print("  3. Implement embedding cache (Redis/memory)")
        print("  4. Consider streaming/incremental processing")
        print()
        print("For Near Real-Time Monitoring (<250ms target):")
        print("  1. Current AWS Bedrock setup is acceptable")
        print("  2. Add embedding caching for common patterns")
        print("  3. Use async processing to avoid blocking")
        print()
        print("For Batch Analysis (no time constraint):")
        print("  ‚úÖ Current implementation is excellent")
        print()
        
        print("="*80 + "\n")
    
    def print_deployment_analysis(self, timings: Dict[str, float]):
        """Analyze deployment scenarios and hardware dependencies."""
        print("\n" + "="*80)
        print("DEPLOYMENT & HARDWARE ANALYSIS")
        print("="*80 + "\n")
        
        print("üñ•Ô∏è  HARDWARE DEPENDENCY BREAKDOWN:\n")
        
        embedding_total = timings['user_embedding_ms'] + timings['model_embedding_ms']
        pca_total = timings['user_pca_ms'] + timings['model_pca_ms']
        erosion_total = timings['erosion_calc_ms']
        
        print(f"{'Component':<35} {'Current':<15} {'CPU Impact':<12} {'GPU Benefit':<12}")
        print("-" * 80)
        print(f"{'AWS Bedrock Embeddings':<35} {embedding_total:>10.2f}ms    {'None':<12} {'None':<12}")
        print(f"  (Network/API latency)")
        print()
        print(f"{'PCA Transformation':<35} {pca_total:>10.2f}ms    {'Low':<12} {'Minimal':<12}")
        print(f"  (NumPy operations)")
        print()
        print(f"{'Erosion Math (R,v,a,C,L,œÅ)':<35} {erosion_total:>10.2f}ms    {'Very Low':<12} {'None':<12}")
        print(f"  (Simple arithmetic)")
        print()
        print("="*80 + "\n")
        
        print("üìä HARDWARE SCENARIOS:\n")
        
        print("1Ô∏è‚É£  CLIENT-SIDE DEPLOYMENT (User's Device):")
        print("   ‚ùå NOT RECOMMENDED with AWS Bedrock")
        print("   ‚Ä¢ Embeddings require API calls ‚Üí network latency varies")
        print("   ‚Ä¢ User's CPU has minimal impact (<5% of total time)")
        print("   ‚Ä¢ User's internet speed is the bottleneck")
        print()
        print("   ‚úÖ ALTERNATIVE: Deploy local embedding model")
        print("   ‚Ä¢ Latency depends on client CPU/GPU")
        print("   ‚Ä¢ Fast CPU: ~50-100ms per turn")
        print("   ‚Ä¢ With GPU: ~20-50ms per turn")
        print()
        
        print("2Ô∏è‚É£  SERVER-SIDE DEPLOYMENT (Your Infrastructure):")
        print("   ‚úÖ RECOMMENDED for current AWS Bedrock setup")
        print("   ‚Ä¢ Consistent latency (~{:.0f}ms per turn)".format(timings['total_ms']))
        print("   ‚Ä¢ Server CPU specs have minimal impact")
        print("   ‚Ä¢ AWS region proximity is most important")
        print()
        print("   Scaling considerations:")
        print("   ‚Ä¢ Each turn is independent ‚Üí easy to parallelize")
        print("   ‚Ä¢ Can handle multiple users simultaneously")
        print("   ‚Ä¢ Bottleneck is AWS API rate limits, not CPU")
        print()
        
        print("3Ô∏è‚É£  HYBRID DEPLOYMENT (Edge + Cloud):")
        print("   ‚ö° OPTIMAL for performance")
        print("   ‚Ä¢ Embeddings: Local edge model (~50ms)")
        print("   ‚Ä¢ PCA + Erosion: Lightweight, runs anywhere (~2ms)")
        print("   ‚Ä¢ Total: ~52ms per turn (5x faster than current)")
        print()
        
        print("="*80 + "\n")
        
        print("‚öôÔ∏è  FINAL ANSWER TO YOUR QUESTIONS:\n")
        print("Q: Is this timing final for client-side deployment?")
        print("A: ‚ùå NO - Current timing includes AWS network latency")
        print("   ‚Ä¢ AWS Bedrock timing varies by:")
        print("     - User's internet speed")
        print("     - Geographic location")
        print("     - AWS region load")
        print("   ‚Ä¢ Client CPU has <5% impact on total time")
        print()
        print("Q: Does it depend on CPU/GPU power?")
        print("A: ‚ö†Ô∏è  PARTIALLY - Depends on deployment model:")
        print()
        print("   With AWS Bedrock (current):")
        print("   ‚Ä¢ CPU/GPU: <5% impact (only PCA + math)")
        print("   ‚Ä¢ Network: >90% impact (AWS API calls)")
        print()
        print("   With Local Embeddings:")
        print("   ‚Ä¢ CPU: Moderate impact (~2x variation)")
        print("     - Fast CPU (i9/Ryzen 9): ~50ms")
        print("     - Slow CPU (i3/Ryzen 3): ~100ms")
        print("   ‚Ä¢ GPU: Significant impact (~5-10x speedup)")
        print("     - With GPU: ~20ms")
        print("     - Without GPU: ~100ms")
        print()
        print("   Math Operations (R,v,a):")
        print("   ‚Ä¢ CPU/GPU: Negligible impact (<1ms always)")
        print("   ‚Ä¢ Runs fast on any modern hardware")
        print()
        
        print("="*80 + "\n")
        
        print("üéØ RECOMMENDATION FOR YOUR USE CASE:\n")
        total_ms = timings['total_ms']
        
        if total_ms < 100:
            print("‚úÖ Current setup is EXCELLENT for real-time alerting!")
            print(f"   End-to-end latency: {total_ms:.0f}ms")
            print("   Ready for production deployment")
        elif total_ms < 250:
            print("‚úÖ Current setup is GOOD for near real-time monitoring")
            print(f"   End-to-end latency: {total_ms:.0f}ms")
            print("   Consider optimization if targeting <100ms")
        else:
            print("‚ö†Ô∏è  Current setup needs optimization for real-time use")
            print(f"   End-to-end latency: {total_ms:.0f}ms")
            print("   Recommend: Deploy local embedding model")
        
        print()
        print("="*80 + "\n")


def main():
    """Main function to run the full pipeline latency test."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Full End-to-End Pipeline Latency Test (including AWS embeddings)',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--user-text', type=str,
                        default="How can I bypass security measures in a system?",
                        help='User message text to test')
    parser.add_argument('--model-text', type=str,
                        default="I cannot and will not provide assistance with bypassing security measures.",
                        help='Model response text to test')
    parser.add_argument('--vsafe-text', type=str,
                        default="I'm designed to be helpful, harmless, and honest.",
                        help='Text to define VSAFE vector')
    
    args = parser.parse_args()
    
    # Locate PCA model
    project_root = os.path.dirname(os.path.dirname(__file__))
    pca_path = os.path.join(project_root, 'models', 'pca_model.pkl')
    
    if not os.path.exists(pca_path):
        print(f"\n‚ùå ERROR: PCA model not found at {pca_path}")
        print("Please run pca_trainer.py first to generate the model.\n")
        return 1
    
    try:
        # Initialize profiler
        profiler = FullPipelineLatencyProfiler(
            pca_model_path=pca_path,
            vsafe_text=args.vsafe_text
        )
        
        # Process a single turn
        timings = profiler.process_single_turn(
            user_text=args.user_text,
            model_text=args.model_text,
            turn_number=1
        )
        
        # Print optimization recommendations
        profiler.print_optimization_recommendations(timings)
        
        # Print deployment analysis
        profiler.print_deployment_analysis(timings)
        
        return 0
        
    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
